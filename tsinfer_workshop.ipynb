{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca9aec2",
   "metadata": {},
   "source": [
    "# ARG inference with tsinfer (final version)\n",
    "\n",
    "## Run this first\n",
    "This will take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97747633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sgkit\n",
    "import tsinfer\n",
    "import msprime\n",
    "import tsdate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install -q jupyterquiz\n",
    "from jupyterquiz import display_quiz\n",
    "from workshop_module import WB_base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca611f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "Many ARG inference methods are available that lie somewhere on a spectrum between inference accuracy and scalability. `tsinfer` is scalable to over 100 000 whole human genomes, does not require any demographic modeling assumptions and supports a variety of data formats. The biggest drawback is that it only produces a single, deterministic estimate of the ARG's topology, without a measure of uncertainty. However, branch lengths are probabilistically estimated by another package, `tsdate`, which has recently been given a major upgrade.\n",
    "\n",
    "This workshop aims to introduce you to ARG inference using the Python packages `tsinfer` and `tsdate`. Working with a subset of the 1000 Genomes Project data in VCF format, we will show how to convert the data into a more useful format and run through the inference process. Finally, we will demonstrate the use of the `tsbrowse` tool for quality controlling an inferred ARG.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Note:</b> For practical reasons, the dataset we are working with is small enough to quickly analyse on an ordinary laptop. For large datasets, where parallel computation on a High Performance Computing Cluster is essential, we have developed a \n",
    "  <a href=\"https://github.com/benjeffery/tsinfer-snakemake/\">Snakemake pipeline</a>\n",
    "  to manage the entire inference process. Feel free to get in touch if you think this will be useful for you!\n",
    "</div>\n",
    "\n",
    "## Outline of the workshop\n",
    "\n",
    "Since other workshops are covering applications, we will mostly focus on the practicalities of working with VCF data, inferring ARGs with `tsinfer` and QCing them:\n",
    "\n",
    "1. Convert the VCF to Zarr format  \n",
    "2. Create the ancestral allele array  \n",
    "3. Inference with `tsinfer`  \n",
    "   3.1. Loading the data  \n",
    "   3.2. Generate ancestors  \n",
    "   3.3. Match ancestors\n",
    "   3.4. Match samples  \n",
    "4. Dating the ARG with `tsdate`  \n",
    "5. Quality control with `tsbrowse`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a297ce0b",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 1. Converting the VCF to Zarr format\n",
    "\n",
    "The VCF format has severe limitations as a genetic data format, especially for large datasets. Extracting data from a particular field is inefficient and it does not lend itself to distributed computing. The **VCF Zarr format**, described in [this preprint](https://www.biorxiv.org/content/10.1101/2024.06.11.598241v3), addresses these limitations, making it easy to slice the data and perform parallel computations efficiently.  \n",
    "\n",
    "**Zarr** is a general format for storing multi-dimensional data, so there are many libraries available to work with it. We will focus on tools specialised for working with VCF Zarr data specifically. The Python package <code>bio2zarr</code> can convert VCF, plink and tskit ARGs into the format, while `sgkit` offers tools for analysing and manipulating the data.\n",
    "\n",
    "As discussed in the [<code>bio2zarr</code> documentation](https://sgkit-dev.github.io/bio2zarr/vcf2zarr/tutorial.html#sec-vcf2zarr-tutorial), converting the VCF to Zarr is a two-step process for moderately-sized datasets. We will do it with the Command Line Interface. First, we **explode** the vcf into the Intermediate Column Format. This separates out the fields (columns) of the VCF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2644f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir data/zarr data/args; vcf2zarr explode -f data/vcf/tgp.vcf.gz data/zarr/tgp.icf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48e7c2",
   "metadata": {},
   "source": [
    "Now we can <code>inspect</code> the ICF we have created, which tells us about the fields in the input VCF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aeb13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "vcf2zarr inspect data/zarr/tgp.icf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff30e80",
   "metadata": {},
   "source": [
    "Minimum and maximum values of numerical fields are shown, so we can see that this is a 1mbp region from 6-7 mbp. At this stage, we choose how we want to encode the ICF to a Zarr by adjusting the **schema**, which allows us to remove fields and make other changes. We don't need to here, but [see the `vcf2zarr` tutorial](https://sgkit-dev.github.io/bio2zarr/vcf2zarr/tutorial.html) if you're interested. \n",
    "\n",
    "The final step is to <code>encode</code> the ICF into a Zarr. Since we are not making any changes to the schema, this is a simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f01a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "vcf2zarr encode -f data/zarr/tgp.icf data/zarr/tgp.zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061e443",
   "metadata": {},
   "source": [
    "We can again use <code>inspect</code> to check our results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd92851",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "vcf2zarr inspect data/zarr/tgp.zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display_quiz(WB_base + \"Q1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac95ce5",
   "metadata": {},
   "source": [
    "Zarr arrays are automatically divided into chunks to aid parallel computation. The genotype matrix is encoded in a 3D array (`call_genotype`). `mask`s are used extensively to exclude certain data (e.g. a `sample_mask` is True for every sample that is excluded)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a83786",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note</b>: for small datasets, a convenience function (<code>convert</code>) is available to do the conversion in one step in Python or with the CLI. For very large datasets, functions are also available to distribute the encoding/decoding across multiple nodes.</div>\n",
    "\n",
    "### 1.1. Exploring the VCF Zarr\n",
    "\n",
    "At the moment, the easiest way to do analyse the data in the VCF Zarr is with the `sgkit` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1237a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = sgkit.load_dataset(\"data/zarr/tgp.zarr\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8630ea5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "  <b>Exercise:</b> The above dataset view is interactive: click on \"Data variables\" to see all the stored arrays. To view one in detail, click the stack symbol on the far right.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec60965",
   "metadata": {},
   "source": [
    "The arrays have name dimensions like `variants` and `samples`, to make it easier to interpret. Large arrays are divided into chunks automatically, which makes it easier to do computations in parallel. The library `xarray` is used to store these arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6641bbfc",
   "metadata": {},
   "source": [
    "\n",
    "You can fetch any of the arrays by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_genotype = ds.call_genotype\n",
    "call_genotype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb523a4",
   "metadata": {},
   "source": [
    "`xarray` objects have names for their dimensions to improve readability. You convert any `xarray` into a simpler `numpy` array using the  `.values` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a71b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = call_genotype.values\n",
    "G[0,0] # diploid genotype at first site of first sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055a3d9a",
   "metadata": {},
   "source": [
    "We can obtain an array of allele counts for each site by summing over all the samples and ploidy: since we have 100 diploid individuals there are up 199 alternate alleles at each site (there are no fixed mutations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29056a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis 0 = sites, axis 1 = samples, axis 2 = ploidy\n",
    "ac = np.sum(ds.call_genotype.values, axis=(1, 2))\n",
    "print(f'Maximum allele count is {ac.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbec25d6",
   "metadata": {},
   "source": [
    "Usually, the two columns of the genotype matrix correspond to `REF` and `ALT` alleles in the VCF. In our case, I've made sure that the `REF` alleles are always ancestral, so `ALT` alleles are always derived. This means that we can plot the derived allele frequency spectrum using the allele counts above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcff8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(ac, bins=50, color='orange', edgecolor='orange')\n",
    "plt.title('Derived allele frequency spectrum')\n",
    "plt.xlabel('Allele count')\n",
    "plt.ylabel('Number of variants')\n",
    "plt.yscale('log')\n",
    "plt.grid(axis='y', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed4c39",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "  <b>Exercise:</b> What is the biggest difference between this AFS and one that you'd expect from a classic Hudson coalescen model? Hint: Rare are alleles are common.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333fc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(WB_base + \"Q2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42f7f5",
   "metadata": {},
   "source": [
    "## 2. Create the ancestral allele array\n",
    "\n",
    "`tsinfer` requires that variants are **bi-allelic**, **phased** and have a **known ancestral state**. Many methods are available to estimate ancestral alleles. In the case of humans, ancestral FASTA sequences, calculated with a tool called Orpheus, are available online. The dataset we are working with has already been polarised with this method, such that the first (reference) allele is always ancestral.\n",
    "\n",
    "To provide `tsinfer` with the ancestral allele array, we start with the `variant_allele` array in the Zarr:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_allele = ds.variant_allele.values\n",
    "variant_allele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec5dad3",
   "metadata": {},
   "source": [
    "In this 2D array, the left column is `REF` and the right column is `ALT`. As mentioned previously, the `REF` alleles are ancestral so we just need to slice them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1158a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestral_state = variant_allele[:,0]  # select all the rows in first column\n",
    "ancestral_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bec15fb",
   "metadata": {},
   "source": [
    "At this stage, we could store this array in the Zarr, but we generally recommend passing it directly to `tsinfer`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6554394",
   "metadata": {},
   "source": [
    "## 3. Inference with `tsinfer`\n",
    "\n",
    "### 3.1. Loading in the data\n",
    "\n",
    "To start inference, we need to load the data into `tsinfer` via the `VariantData` class, which is just a wrapper of a zarr. To specify the ancestral states, we can either point it to a variable in the Zarr, or provide the array directly; we will do the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_data = tsinfer.VariantData(\"data/zarr/tgp.zarr\", ancestral_state=ancestral_state)\n",
    "print(f'There are are {variant_data.num_sites} in the zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe3f725",
   "metadata": {},
   "source": [
    "We can still access the data in the zarr, which is accessible in `variant_data.data`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7269767",
   "metadata": {},
   "source": [
    "### 3.2. Generate ancestors\n",
    "\n",
    "The first step of `tsinfer`, ancestor generation, uses the genotype data to estimate the ancestral haplotypes that the samples inherited from. For a given set of inference sites with the same allele frequency and genotypes, which we call **focal sites**, we create an ancestor that carries the derived mutation, extending to the left and right until a stopping criterion is reached.\n",
    "\n",
    "The result of ancestor generation is an object called `AncestorData`, which contains all the inferred ancestral haplotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_data = tsinfer.generate_ancestors(variant_data, progress_monitor=True)\n",
    "print(f'Generated {anc_data.num_ancestors} ancestors from {variant_data.num_sites} sites')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f60c04",
   "metadata": {},
   "source": [
    "For what follows, ancestors need to have a measure of relative age, so that we can match their haplotypes against each other. The default is to use **allele frequency as a proxy of relative age**, since we expect rare mutations to have occurred more recently than common ones. \n",
    "\n",
    "We usually don't do any QC or analysis of the ancestor data at this stage except for development purposes.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note</b>: Ancestor generation is usually much faster than the subsequent steps (ancestor matching and sample matching). The only potential issue is that it can be RAM intensive for large datasets, since the 1-bit encoded genotype matrix needs to fit in RAM. </div>\n",
    "\n",
    "### 3.3. Match ancestors\n",
    "\n",
    "We next need to match the ancestral haplotypes against each other to form an **ancestors tree sequence/ARG**. This ARG captures all the copying patterns between the ancestors, but does not include the sample haplotypes. `tsinfer` uses the [Li and Stephens (2003) Hidden Markov Model (HMM)](https://pubmed.ncbi.nlm.nih.gov/14704198/) for matching. It has two main inputs in our implementation: `recombination_rate` and `mismatch_ratio`. We will discuss these in detail in the next section, because we **strongly advise leaving them both at default for this step, as in the call below**. \n",
    "\n",
    "The `match_ancestors` function needs both the `VariantData` and `AncestorData` to run, so the call looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0075173",
   "metadata": {},
   "outputs": [],
   "source": [
    "ancestors_arg = tsinfer.match_ancestors(variant_data, anc_data, progress_monitor=False)\n",
    "ancestors_arg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360c19c4",
   "metadata": {},
   "source": [
    "Curiously, there are 3168 sample nodes, one for each ancestor. This is because we haven't added the actual samples yet. We usually don't QC the ancestor ARG directly, so we will proceed to the final stage.\n",
    "\n",
    "### 3.4. Match samples\n",
    "\n",
    "\n",
    "The final inference step is to match the sample haplotypes against the ancestors ARG, which will complete the ARG topology. To enable the use of the mismatch parameter discussed below, you must specify a **recombination rate**, which can be either be:\n",
    "\n",
    "1. A floating point recombination rate $\\rho$ per unit length of genome\n",
    "2. A recombination map stored as an `msprime.RateMap` object.\n",
    "\n",
    "If you have a recombination rate map available, as is the case for humans, it is worth using. We have provided the HapMap for chromosome 20 to use with our data, so we need to use `msprime` to load it in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e963267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hapmap_path = \"data/hapmap/genetic_map_Hg38_chr20.txt\"\n",
    "rate_map = msprime.RateMap.read_hapmap(hapmap_path, position_col=1, rate_col=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401150f3",
   "metadata": {},
   "source": [
    "Whenever a recombination rate is provided, you can also specify a `mismatch_ratio`, which determines the balance between recombination events and recurrent mutations. If a mismatch is encountered at a site between otherwise closely matching haplotypes, the HMM will\n",
    "\n",
    "- Almost always adds a recombination event if `mismatch_ratio ~ 0` (default, i.e. it is infinitesimal)\n",
    "- Be equally likely to add a recombination event or recurrent mutation if `mismatch_ratio = 1`\n",
    "- Be two times more likely to add a recurrent mutation if `mismatch_ratio = 2`\n",
    "\n",
    "We advise setting mismatch to zero in the first instance, which makes inference much faster and results in little to no recurrent mutations being added. If you do set it to a non-zero value, it is best to make it very small ($\\leq 10^{-3}$), otherwise an excessively many recurrent mutations will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819fd0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg = tsinfer.match_samples(variant_data,\n",
    "                                     ancestors_arg,\n",
    "                                     recombination_rate=rate_map,\n",
    "                                     mismatch_ratio=0, \n",
    "                                     progress_monitor=True,\n",
    "                                     )\n",
    "arg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e2af6",
   "metadata": {},
   "source": [
    "#### One recurrent mutation?\n",
    "Notice that one site has more than 1 mutations (since `num_mutations` - `num_sites` = 1). What is up with this site? To find out, we can use the `sites()` iterator (the slow way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in arg.sites():\n",
    "    mutations = site.mutations\n",
    "    if len(mutations) > 1: #recurrent\n",
    "        for mut in mutations:\n",
    "            print(mut)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d81666",
   "metadata": {},
   "source": [
    "It's a recurrent mutation! This can rarely happen, since we provided a recombination map but the mismatch ratio is slightly above zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2e721",
   "metadata": {},
   "source": [
    "#### Lots more sites vs. the ancestor trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad70e9",
   "metadata": {},
   "source": [
    "By default, `match_samples` post-processes the final ARG to remove some artefacts of the algorithm and simplify without removing unary nodes. This step can be disabled, but for most users it is best to stick to the default behaviour.\n",
    "\n",
    "Notice that the **number of sites has increased** after sample matching. This is because there are often sites in the Zarr which are not suitable for inference; `match_samples` adds them back to the ARG by parsimony by default, which can also be disabled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbdf557",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {arg.num_sites - ancestors_arg.num_sites} new sites added by match samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6a086c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "  <b>Exercise:</b> In this case, all the sites added have the same property. What are they? Hint: the previous challenge question might help.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99775abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(WB_base + \"Q3.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e9d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "crazy_arg = #insert here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3092c",
   "metadata": {},
   "source": [
    "\n",
    "### 4. Dating the ARG with `tsdate`\n",
    "\n",
    "The ARG we have made still has uncalibrated branch lengths: all the nodes have an age between 0 and ~1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685ccd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = arg.tables.nodes.time\n",
    "print(f'Node times are between {min(times)} and {max(times)} in {arg.time_units} time units')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e3ca3",
   "metadata": {},
   "source": [
    "The `tsdate` package uses an expectation-propagation (EP) algorithm to calibrate the ARG's branch lengths to units of generations. First, we need to **pre-process** the ARG, then provide a **mutation rate** to `tsdate` for your species. For humans, we usually use $1.29\\times 10^{-8}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_arg = tsdate.preprocess_ts(arg)\n",
    "dated_arg = tsdate.date(preproc_arg, mutation_rate=1.29e-8, progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eeca37",
   "metadata": {},
   "source": [
    "Checking the node ages again,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa270a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "times = dated_arg.tables.nodes.time\n",
    "\n",
    "print(f'Node times are between {min(times)} and {max(times)} in {dated_arg.time_units} time units')\n",
    "dated_arg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568bc32e",
   "metadata": {},
   "source": [
    "As we can see from the above, the time units are now calibrated to generations. **All the singletons have been dated too**! For what follows, let's save it to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dated_arg.dump('data/args/dated.trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f5625",
   "metadata": {},
   "source": [
    "## 5. Quality control with `tsbrowse`\n",
    "\n",
    "Since large ARGs are too complex to fully visualise as a graph or tree sequence, quality control can be difficult. In our experience, it is dangerous to solely rely on summary statistics and summary plots (e.g. of node ages), since many issues can be missed this way.\n",
    "\n",
    "To help QC inferred ARGs, we developed a web app called **`tsbrowse`** (led by Savita Karthikeyan) to interactively visualise nodes, edges and various other data. The app works for all major ARG inference methods, provided they are in `tskit` format. \n",
    "\n",
    "Using `tsbrowse` is a two step process. First, we `preprocess` the ARG into a compressed `tsbrowse` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7d7181",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m tsbrowse preprocess data/args/dated.trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6e866",
   "metadata": {},
   "source": [
    "Then we need to start the app server. The `--show` command should make it open in your browser when it's ready. If that doesn't work, just copy-paste `http://localhost:1111/` in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f6e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python -m tsbrowse serve --port 1112 --show data/args/dated.tsbrowse \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bad01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz(WB_base + \"Q4.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91266713",
   "metadata": {},
   "source": [
    "### Things to look out for in `tsbrowse`\n",
    "\n",
    "1. **Mutations**: Are there any regions with large gaps / low site density? These can bias downstream statistics.\n",
    "2. **Mutations**: In log scale, do you generally see fewer mutations as age increases? Sometimes, ARG inference errors can cause an excessive number of very old mutations (>100 000 generations)\n",
    "3. **Nodes**: The plot should generally be L-shaped, because older ancestors should tend to be shorter than younger ones.\n",
    "4. **Edges**: Look for artefacts (such as regions with exceptionally long edges).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a9e30c",
   "metadata": {},
   "source": [
    "## A few exploratory plots\n",
    "\n",
    "Once we are satisfied with the quality control results, we can start analysing our results. For example, let's calculate the branch-mode diversity along the genome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bdb762-536c-42f1-8468-9ce2e670d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows have to go from 0 to sequence_length\n",
    "windows = np.arange(6e6, 7e6, 1e4)\n",
    "all_windows = np.concatenate([[0], windows, [dated_arg.sequence_length]])\n",
    "pi_window = dated_arg.diversity(mode=\"branch\",windows=all_windows)\n",
    "#plot diversity along genome in windows\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot((all_windows[:-1] + all_windows[1:]) / 2,\n",
    "            pi_window,\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            color='blue')\n",
    "plt.title('Branch-mode diversity along genome')\n",
    "plt.xlabel('Genomic Position (bp)')\n",
    "plt.ylabel('Avg. branch length between samples')\n",
    "plt.xlim(6e6, 7e6)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5e5eb6",
   "metadata": {},
   "source": [
    "`tsdate` now automatically estimates mutation ages and adds the results to the mutations table. Let's see what the range of mutation ages is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dated_arg.tables.mutations.time\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(x, bins=50, color='green', edgecolor='black')\n",
    "plt.title('Histogram of mutation ages')\n",
    "plt.xlabel('Mutation age (generations)')\n",
    "plt.ylabel('Number of mutations')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de4a0b9",
   "metadata": {},
   "source": [
    "### A note about age estimates\n",
    "\n",
    "`tsdate` now uses a Bayesian algorithm to estimate the age of nodes (and hence mutations). Conveniently, the posterior distribution of age estimates follows a Gamma distribution, and the mean and variance are stored in the metadata as `mn` and `vr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30314bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation = dated_arg.mutation(50)\n",
    "print(f'Mutation metadata: {mutation.metadata}')\n",
    "node = dated_arg.node(mutation.node)\n",
    "print(f'Node metadata: {node.metadata}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08bcd12",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsinfer-workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
